{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sweetviz as sv\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "import optuna\n",
    "from tabpfn import TabPFNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_domain = pd.read_csv(\"../data/dataset.csv\")\n",
    "data_img = pd.read_csv(\"../data/features(1).csv\")\n",
    "data_domain_valid = pd.read_csv(\"../data/dataset_valid.csv\")\n",
    "data_img_valid = pd.read_csv(\"../data/features_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data_domain, data_img, on=\"id\", how=\"inner\")\n",
    "data_valid = pd.merge(data_domain_valid, data_img_valid, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'latitude_min', 'longitude_min', 'latitude_max', 'longitude_max',\n",
       "       'sand', 'coral_algae', 'rock', 'seagrass', 'microalgal_mats', 'rubble',\n",
       "       'sand_rate', 'coral_algae_rate', 'rock_rate', 'seagrass_rate',\n",
       "       'microalgal_mats_rate', 'rubble_rate', 'seagrass_overlap', 'r_sum',\n",
       "       'r_mean', 'r_var', 'g_sum', 'g_mean', 'g_var', 'b_sum', 'b_mean',\n",
       "       'b_var', 'hog_sum', 'hog_mean', 'hog_var', 'sift_sum', 'sift_mean',\n",
       "       'sift_var'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_location = ['latitude_min', 'longitude_min', 'latitude_max', 'longitude_max']\n",
    "columns_base = ['sand', 'coral_algae', 'rock', 'seagrass', 'microalgal_mats', 'rubble']\n",
    "columns_rate = ['sand_rate', 'coral_algae_rate', 'rock_rate', 'seagrass_rate', 'microalgal_mats_rate', 'rubble_rate']\n",
    "columns_rgb = ['r_sum', 'r_mean', 'r_var', 'g_sum', 'g_mean', 'g_var', 'b_sum', 'b_mean', 'b_var', 'hog_sum']\n",
    "columns_hog = ['hog_sum', 'hog_mean', 'hog_var']\n",
    "columns_sift = ['sift_sum', 'sift_mean', 'sift_var']\n",
    "target = \"seagrass_overlap\"\n",
    "target_binary = \"target_binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target_binary'] = data[target].apply(lambda x: 0 if x == 0 else 1)\n",
    "data_valid['target_binary'] = data_valid[target].apply(lambda x: 0 if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリカル作成\n",
    "for col in ['sand', 'coral_algae', 'rock', 'seagrass', 'microalgal_mats', 'rubble']:\n",
    "    data[col + '_onehot'] = data[col].apply(lambda x: 1 if x == 1 else 0)\n",
    "for col in ['sand', 'coral_algae', 'rock', 'seagrass', 'microalgal_mats', 'rubble']:\n",
    "    data_valid[col + '_onehot'] = data_valid[col].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置情報×ドメイン作成\n",
    "categories = ['seagrass', 'seagrass_rate', 'coral_algae', 'rock', 'microalgal_mats', 'rubble']\n",
    "for category in categories:\n",
    "    data[f'latitude×{category}'] = ((data['latitude_min'] + data['latitude_max']) / 2) * data[category]\n",
    "    data[f'longitude×{category}'] = ((data['longitude_min'] + data['longitude_max']) / 2) * data[category]\n",
    "for category in categories:\n",
    "    data_valid[f'latitude×{category}'] = ((data_valid['latitude_min'] + data_valid['latitude_max']) / 2) * data_valid[category]\n",
    "    data_valid[f'longitude×{category}'] = ((data_valid['longitude_min'] + data_valid['longitude_max']) / 2) * data_valid[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seagrass関係をexponentialize\n",
    "data['seagrass_exp'] = np.exp(data['seagrass'])\n",
    "data['seagrass_rate_exp'] = np.exp(data['seagrass_rate'])\n",
    "data_valid['seagrass_exp'] = np.exp(data_valid['seagrass'])\n",
    "data_valid['seagrass_rate_exp'] = np.exp(data_valid['seagrass_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置情報×画像\n",
    "features = ['r_mean', 'g_mean', 'b_mean', 'hog_mean', 'sift_mean']\n",
    "\n",
    "data['longitude_avg'] = (data['longitude_min'] + data['longitude_max']) / 2\n",
    "data['latitude_avg'] = (data['latitude_min'] + data['latitude_max']) / 2\n",
    "\n",
    "for feature in features:\n",
    "    data[f'longitude_avg×{feature}'] = data['longitude_avg'] * data[feature]\n",
    "    data[f'latitude_avg×{feature}'] = data['latitude_avg'] * data[feature]\n",
    "\n",
    "data_valid['longitude_avg'] = (data_valid['longitude_min'] + data_valid['longitude_max']) / 2\n",
    "data_valid['latitude_avg'] = (data_valid['latitude_min'] + data_valid['latitude_max']) / 2\n",
    "\n",
    "for feature in features:\n",
    "    data_valid[f'longitude_avg×{feature}'] = data_valid['longitude_avg'] * data_valid[feature]\n",
    "    data_valid[f'latitude_avg×{feature}'] = data_valid['latitude_avg'] * data_valid[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドメイン×画像\n",
    "features = ['r_mean', 'g_mean', 'b_mean', 'hog_mean', 'sift_mean']\n",
    "categories = ['sand', 'coral_algae', 'rock', 'seagrass', 'microalgal_mats', 'rubble']\n",
    "\n",
    "for feature in features:\n",
    "    for category in categories:\n",
    "        new_column_name = f'{feature}×{category}'\n",
    "        data[new_column_name] = data[feature] * data[category]\n",
    "\n",
    "for feature in features:\n",
    "    for category in categories:\n",
    "        new_column_name = f'{feature}×{category}'\n",
    "        data_valid[new_column_name] = data_valid[feature] * data_valid[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_base = data[columns_base]\n",
    "X_data_base_valid = data_valid[columns_base]\n",
    "\n",
    "X_data_rate_img = data[columns_rate + columns_rgb + columns_hog + columns_sift]\n",
    "X_data_rate_img_valid = data_valid[columns_rate + columns_rgb + columns_hog + columns_sift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 教師無しクラスタリング\n",
    "\n",
    "kmeans_3 = KMeans(n_clusters=3, random_state=0).fit(X_data_base)\n",
    "data['base_3'] = kmeans_3.labels_\n",
    "kmeans_3_valid = KMeans(n_clusters=3, random_state=0).fit(X_data_base_valid)\n",
    "data_valid['base_3'] = kmeans_3_valid.predict(X_data_base_valid)\n",
    "\n",
    "kmeans_5 = KMeans(n_clusters=5, random_state=0).fit(X_data_base)\n",
    "data['base_5'] = kmeans_5.labels_\n",
    "kmeans_5_valid = KMeans(n_clusters=5, random_state=0).fit(X_data_base_valid)\n",
    "data_valid['base_5'] = kmeans_5_valid.predict(X_data_base_valid)\n",
    "\n",
    "kmeans_5_ = KMeans(n_clusters=5, random_state=0).fit(X_data_rate_img)\n",
    "data['rate_img__5'] = kmeans_5_.labels_\n",
    "kmeans_5_valid_ = KMeans(n_clusters=5, random_state=0).fit(X_data_rate_img_valid)\n",
    "data_valid['rate_img_5'] = kmeans_5_valid_.predict(X_data_rate_img_valid)\n",
    "\n",
    "kmeans_7 = KMeans(n_clusters=7, random_state=0).fit(X_data_rate_img)\n",
    "data['rate_img_7'] = kmeans_7.labels_\n",
    "kmeans_7_valid = KMeans(n_clusters=7, random_state=0).fit(X_data_rate_img_valid)\n",
    "data_valid['rate_img_7'] = kmeans_7_valid.predict(X_data_rate_img_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_location = ['latitude_min', 'longitude_min', 'latitude_max', 'longitude_max']\n",
    "columns_base = ['sand', 'coral_algae', 'rock', 'seagrass', 'microalgal_mats', 'rubble']\n",
    "columns_rate = ['sand_rate', 'coral_algae_rate', 'rock_rate', 'seagrass_rate', 'microalgal_mats_rate', 'rubble_rate']\n",
    "columns_rgb = ['r_sum', 'r_mean', 'r_var', 'g_sum', 'g_mean', 'g_var', 'b_sum', 'b_mean', 'b_var', 'hog_sum']\n",
    "columns_hog = ['hog_sum', 'hog_mean', 'hog_var']\n",
    "columns_sift = ['sift_sum', 'sift_mean', 'sift_var']\n",
    "columns_onehot = ['sand_onehot', 'coral_algae_onehot', 'rock_onehot', 'seagrass_onehot', 'microalgal_mats_onehot', 'rubble_onehot']\n",
    "columns_loc_base = ['latitude×seagrass', 'longitude×seagrass', 'latitude×seagrass_rate', 'longitude×seagrass_rate', 'latitude×coral_algae', 'longitude×coral_algae', 'latitude×rock', 'longitude×rock', 'latitude×microalgal_mats', 'longitude×microalgal_mats', 'latitude×rubble', 'longitude×rubble']\n",
    "columns_exp = ['seagrass_exp', 'seagrass_rate_exp']\n",
    "columns_loc_img = ['longitude_avg×r_mean', 'latitude_avg×r_mean', 'longitude_avg×g_mean', 'latitude_avg×g_mean', 'longitude_avg×b_mean', 'latitude_avg×b_mean', 'longitude_avg×hog_mean', 'latitude_avg×hog_mean', 'longitude_avg×sift_mean', 'latitude_avg×sift_mean']\n",
    "columns_kmeans = ['base_3', 'base_5', 'rate_img_5', 'rate_img_7']\n",
    "target = \"seagrass_overlap\"\n",
    "target_binary = \"target_binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_location = ['latitude_min', 'longitude_min', 'latitude_max', 'longitude_max']\n",
    "columns_base = ['sand', 'coral_algae', 'rock', 'seagrass', 'microalgal_mats', 'rubble']\n",
    "columns_rate = ['sand_rate', 'coral_algae_rate', 'rock_rate', 'seagrass_rate', 'microalgal_mats_rate', 'rubble_rate']\n",
    "columns_rgb = ['r_sum', 'r_mean', 'r_var', 'g_sum', 'g_mean', 'g_var', 'b_sum', 'b_mean', 'b_var']\n",
    "columns_hog = ['hog_sum', 'hog_mean', 'hog_var']\n",
    "columns_sift = ['sift_sum', 'sift_mean', 'sift_var']\n",
    "columns_onehot = ['sand_onehot', 'coral_algae_onehot', 'rock_onehot', 'seagrass_onehot', 'microalgal_mats_onehot', 'rubble_onehot']\n",
    "columns_loc_base = ['latitude×seagrass', 'longitude×seagrass', 'latitude×seagrass_rate', 'longitude×seagrass_rate', 'latitude×coral_algae', 'longitude×coral_algae', 'latitude×rock', 'longitude×rock', 'latitude×microalgal_mats', 'longitude×microalgal_mats', 'latitude×rubble', 'longitude×rubble']\n",
    "columns_exp = ['seagrass_exp', 'seagrass_rate_exp']\n",
    "columns_loc_img = ['longitude_avg×r_mean', 'latitude_avg×r_mean', 'longitude_avg×g_mean', 'latitude_avg×g_mean', 'longitude_avg×b_mean', 'latitude_avg×b_mean', 'longitude_avg×hog_mean', 'latitude_avg×hog_mean', 'longitude_avg×sift_mean', 'latitude_avg×sift_mean']\n",
    "columns_base_img =['r_mean×sand',\n",
    "       'r_mean×coral_algae', 'r_mean×rock', 'r_mean×seagrass',\n",
    "       'r_mean×microalgal_mats', 'r_mean×rubble', 'g_mean×sand',\n",
    "       'g_mean×coral_algae', 'g_mean×rock', 'g_mean×seagrass',\n",
    "       'g_mean×microalgal_mats', 'g_mean×rubble', 'b_mean×sand',\n",
    "       'b_mean×coral_algae', 'b_mean×rock', 'b_mean×seagrass',\n",
    "       'b_mean×microalgal_mats', 'b_mean×rubble', 'hog_mean×sand',\n",
    "       'hog_mean×coral_algae', 'hog_mean×rock', 'hog_mean×seagrass',\n",
    "       'hog_mean×microalgal_mats', 'hog_mean×rubble', 'sift_mean×sand',\n",
    "       'sift_mean×coral_algae', 'sift_mean×rock', 'sift_mean×seagrass',\n",
    "       'sift_mean×microalgal_mats', 'sift_mean×rubble']\n",
    "columns_kmeans = ['base_3', 'base_5', 'rate_img_5', 'rate_img_7']\n",
    "target = \"seagrass_overlap\"\n",
    "target_binary = \"target_binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_a = columns_location + columns_base + columns_rate + columns_rgb + columns_hog + columns_sift + columns_onehot + columns_kmeans\n",
    "columns_b = columns_base + columns_rate + columns_rgb + columns_hog + columns_sift + columns_onehot + columns_kmeans\n",
    "columns_c = columns_base + columns_rgb + columns_hog + columns_sift\n",
    "columns_d = columns_loc_base + columns_onehot + columns_rgb + columns_hog + columns_sift\n",
    "columns_e = columns_exp + columns_base_img + columns_kmeans\n",
    "columns_f = columns_loc_img + columns_rate\n",
    "columns_g = columns_base + columns_rate + columns_rgb + columns_hog + columns_sift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics and Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    threshold: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    予測値と実際の値から混同行列を生成する関数。\n",
    "\n",
    "    引数:\n",
    "    - y_true: np.ndarray, テストデータセットの実際のクラスラベル。\n",
    "    - y_pred: np.ndarray, テストデータセットに対する予測確率。\n",
    "    - threshold: float, 予測確率をクラスラベルに変換するための閾値。\n",
    "\n",
    "    戻り値:\n",
    "    - np.ndarray: 生成された混同行列。\n",
    "    \"\"\"\n",
    "    y_pred_label = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred_label)\n",
    "    \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_lightgbm_stratified_kfold(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    columns_feature: List[str],\n",
    "    target_binary: List[str],  # この実装では使用しないが、型情報を含める\n",
    "    param: Dict[str, any],\n",
    "    n_splits: int = 5,   \n",
    ") -> Tuple[List[lgb.Booster], List[float]]:\n",
    "    \"\"\"\n",
    "    Stratified k-foldクロスバリデーションを使用してLightGBMモデルを訓練する関数。\n",
    "    非均衡データを考慮して、各クラスの割合を保持する。\n",
    "\n",
    "    引数:\n",
    "    - X: pandas DataFrame, 特徴量データ。\n",
    "    - y: pandas Series, 目的変数データ。\n",
    "    - columns_feature: list, 特徴量のカラム名のリスト。\n",
    "    - target_binary: list, 目的変数のカラム名のリスト（この関数では使用しないが、一貫性のために残す）。\n",
    "    - param: dict, LightGBMモデルのパラメータ。\n",
    "    - n_splits: int, クロスバリデーションの分割数。\n",
    "\n",
    "    戻り値:\n",
    "    - Tuple[List[lgb.Booster], List[float]]: 訓練済みLightGBMモデルのリストと各foldのlog lossスコアのリスト。\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    models: List[lgb.Booster] = []\n",
    "    scores: List[float] = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = (\n",
    "            X.iloc[train_index][columns_feature],\n",
    "            X.iloc[test_index][columns_feature],\n",
    "        )\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "        gbm: lgb.Booster = lgb.train(\n",
    "            param,\n",
    "            lgb_train,\n",
    "            valid_sets=[lgb_train, lgb_eval],\n",
    "        )\n",
    "\n",
    "        y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "        score: float = log_loss(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "        models.append(gbm)\n",
    "\n",
    "    return models, scores\n",
    "\n",
    "def predict_with_lightgbm(\n",
    "    models: List[lgb.Booster],\n",
    "    X_test: pd.DataFrame,\n",
    "    columns_feature: List[str]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    訓練済みLightGBMモデルのリストを使用してテストデータセットの予測を行う関数。\n",
    "\n",
    "    引数:\n",
    "    - models: List[lgb.Booster], 訓練済みLightGBMモデルのリスト。\n",
    "    - X_test: pandas DataFrame, テストデータセット。\n",
    "    - columns_feature: list, 特徴量のカラム名のリスト。\n",
    "\n",
    "    戻り値:\n",
    "    - np.ndarray: テストデータセットの予測値の平均値。\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        y_pred = model.predict(X_test[columns_feature], num_iteration=model.best_iteration)\n",
    "        predictions.append(y_pred)\n",
    "    \n",
    "    predictions_mean = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return predictions_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lightgbm(trial, X, y, columns_feature, n_splits=5):\n",
    "    \"\"\"\n",
    "    Optunaの試行に対する目的関数。LightGBMのハイパーパラメータを探索する。\n",
    "\n",
    "    引数:\n",
    "    - trial: optuna.trial.Trial オブジェクト\n",
    "    - X, y: 訓練データ\n",
    "    - columns_feature: 特徴量のカラム名のリスト\n",
    "    - n_splits: クロスバリデーションの分割数\n",
    "\n",
    "    戻り値:\n",
    "    - 試行の平均log lossスコア\n",
    "    \"\"\"\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }\n",
    "\n",
    "    _, scores = train_lightgbm_stratified_kfold(\n",
    "        X, y, columns_feature, [], param, n_splits\n",
    "    )\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def optimize_hyperparameters_lightgbm(\n",
    "    X, y, columns_feature, n_trials=100, n_splits=5, timeout=300\n",
    "):\n",
    "    \"\"\"\n",
    "    Optunaを使用してLightGBMモデルのハイパーパラメータを最適化する。\n",
    "\n",
    "    引数:\n",
    "    - X, y: 訓練データ\n",
    "    - columns_feature: 特徴量のカラム名のリスト\n",
    "    - n_trials: 試行回数の上限\n",
    "    - n_splits: クロスバリデーションの分割数\n",
    "    - timeout: 探索にかける時間の上限（秒）\n",
    "\n",
    "    戻り値:\n",
    "    - 最適なハイパーパラメータの辞書\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective_lightgbm(trial, X, y, columns_feature, n_splits),\n",
    "        n_trials=n_trials,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.value}\")\n",
    "    print(f\"Best params: {study.best_trial.params}\")\n",
    "\n",
    "    return study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_hyperparameters_lightgbm(data_valid[columns_a], data_valid[target_binary], columns_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_lightgbm, scores_lightgbm = train_lightgbm_stratified_kfold(data[columns_a], data[target_binary], columns_base, target_binary, param)\n",
    "#NOTE: Define param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lightgbm = predict_with_lightgbm(models_lightgbm, data_valid[columns_a], columns_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1762,    3],\n",
       "       [ 137,    0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_confusion_matrix(data_valid[target_binary], y_pred_lightgbm, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_stratified_kfold(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    columns_feature: List[str],\n",
    "    target_binary: List[str],  # この実装では使用しないが、型情報を含める\n",
    "    param: Dict[str, any],\n",
    "    n_splits: int = 5,\n",
    ") -> Tuple[List[xgb.Booster], List[float]]:\n",
    "    \"\"\"\n",
    "    Stratified k-foldクロスバリデーションを使用してXGBoostモデルを訓練する関数。\n",
    "    非均衡データを考慮して、各クラスの割合を保持する。\n",
    "\n",
    "    引数:\n",
    "    - X: pandas DataFrame, 特徴量データ。\n",
    "    - y: pandas Series, 目的変数データ。\n",
    "    - columns_feature: list, 特徴量のカラム名のリスト。\n",
    "    - target_binary: list, 目的変数のカラム名のリスト（この関数では使用しないが、一貫性のために残す）。\n",
    "    - param: dict, XGBoostモデルのパラメータ。\n",
    "    - n_splits: int, クロスバリデーションの分割数。\n",
    "\n",
    "    戻り値:\n",
    "    - Tuple[List[xgb.Booster], List[float]]: 訓練済みXGBoostモデルのリストと各foldのlog lossスコアのリスト。\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    models: List[xgb.Booster] = []\n",
    "    scores: List[float] = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = (\n",
    "            X.iloc[train_index][columns_feature],\n",
    "            X.iloc[test_index][columns_feature],\n",
    "        )\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        bst: xgb.Booster = xgb.train(\n",
    "            param,\n",
    "            dtrain,\n",
    "            num_boost_round=param.get(\"num_boost_round\", 100),\n",
    "            evals=[(dtrain, \"train\"), (dtest, \"eval\")],\n",
    "            # early_stopping_rounds=param.get(\"early_stopping_rounds\", 10),\n",
    "            # verbose_eval=param.get(\"verbose_eval\", 50),\n",
    "        )\n",
    "\n",
    "        y_pred = bst.predict(dtest)\n",
    "        score: float = log_loss(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "        models.append(bst)\n",
    "\n",
    "    return models, scores\n",
    "\n",
    "\n",
    "def predict_with_xgboost(models: List[xgb.Booster], X_test: pd.DataFrame, columns_feature: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    訓練済みXGBoostモデルのリストを使用してテストデータセットの予測を行う関数。\n",
    "\n",
    "    引数:\n",
    "    - models: List[xgb.Booster], 訓練済みXGBoostモデルのリスト。\n",
    "    - X_test: pandas DataFrame, テストデータセット。\n",
    "    - columns_feature: list, 特徴量のカラム名のリスト。\n",
    "\n",
    "    戻り値:\n",
    "    - np.ndarray: テストデータセットの予測値の平均値。\n",
    "    \"\"\"\n",
    "    # 各モデルからの予測値を格納するリスト\n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        dtest = xgb.DMatrix(X_test[columns_feature])\n",
    "        y_pred = model.predict(dtest)\n",
    "        predictions.append(y_pred)\n",
    "    \n",
    "    # 予測値の平均を計算\n",
    "    predictions_mean = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return predictions_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgboost(trial, X, y, columns_feature, n_splits=5):\n",
    "    \"\"\"\n",
    "    Optunaの試行に対する目的関数。XGBoostのハイパーパラメータを探索する。\n",
    "\n",
    "    引数:\n",
    "    - trial: optuna.trial.Trial オブジェクト\n",
    "    - X, y: 訓練データ\n",
    "    - columns_feature: 特徴量のカラム名のリスト\n",
    "    - n_splits: クロスバリデーションの分割数\n",
    "\n",
    "    戻り値:\n",
    "    - 試行の平均log lossスコア\n",
    "    \"\"\"\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"eta\": trial.suggest_loguniform(\"eta\", 1e-3, 0.1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-3, 10.0),\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-3, 10.0),\n",
    "    }\n",
    "\n",
    "    _, scores = train_xgboost_stratified_kfold(\n",
    "        X, y, columns_feature, [], param, n_splits\n",
    "    )\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def optimize_hyperparameters_xgboost(\n",
    "    X, y, columns_feature, n_trials=100, n_splits=5, timeout=300\n",
    "):\n",
    "    \"\"\"\n",
    "    Optunaを使用してXGBoostモデルのハイパーパラメータを最適化する。\n",
    "\n",
    "    引数:\n",
    "    - X, y: 訓練データ\n",
    "    - columns_feature: 特徴量のカラム名のリスト\n",
    "    - n_trials: 試行回数の上限\n",
    "    - n_splits: クロスバリデーションの分割数\n",
    "    - timeout: 探索にかける時間の上限（秒）\n",
    "\n",
    "    戻り値:\n",
    "    - 最適なハイパーパラメータの辞書\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective_xgboost(trial, X, y, columns_feature, n_splits),\n",
    "        n_trials=n_trials,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.value}\")\n",
    "    print(f\"Best params: {study.best_trial.params}\")\n",
    "\n",
    "    return study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_catboost_stratified_kfold(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    columns_feature: List[str],\n",
    "    target_binary: List[str],  # この実装では使用しないが、型情報を含める\n",
    "    param: Dict[str, any],\n",
    "    n_splits: int = 5,\n",
    ") -> Tuple[List[CatBoostClassifier], List[float]]:\n",
    "    \"\"\"\n",
    "    Stratified k-foldクロスバリデーションを使用してCatBoostモデルを訓練する関数。\n",
    "    非均衡データを考慮して、各クラスの割合を保持する。\n",
    "\n",
    "    引数:\n",
    "    - X: pandas DataFrame, 特徴量データ。\n",
    "    - y: pandas Series, 目的変数データ。\n",
    "    - columns_feature: list, 特徴量のカラム名のリスト。\n",
    "    - target_binary: list, 目的変数のカラム名のリスト（この関数では使用しないが、一貫性のために残す）。\n",
    "    - param: dict, CatBoostモデルのパラメータ。\n",
    "    - n_splits: int, クロスバリデーションの分割数。\n",
    "\n",
    "    戻り値:\n",
    "    - Tuple[List[CatBoostClassifier], List[float]]: 訓練済みCatBoostモデルのリストと各foldのlog lossスコアのリスト。\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    models: List[CatBoostClassifier] = []\n",
    "    scores: List[float] = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = (\n",
    "            X.iloc[train_index][columns_feature],\n",
    "            X.iloc[test_index][columns_feature],\n",
    "        )\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        train_pool = Pool(X_train, y_train)\n",
    "        test_pool = Pool(X_test, y_test)\n",
    "\n",
    "        model = CatBoostClassifier(**param)\n",
    "        model.fit(\n",
    "            train_pool,\n",
    "            eval_set=test_pool,\n",
    "            # verbose=50,\n",
    "            # early_stopping_rounds=100,\n",
    "            use_best_model=True,\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict_proba(X_test)[:, 1]\n",
    "        score = log_loss(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "        models.append(model)\n",
    "\n",
    "    return models, scores\n",
    "\n",
    "\n",
    "def predict_with_catboost(\n",
    "    models: List[CatBoostClassifier], X_test: pd.DataFrame, columns_feature: List[str]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    訓練済みCatBoostモデルのリストを使用してテストデータセットの予測を行う関数。\n",
    "\n",
    "    引数:\n",
    "    - models: List[CatBoostClassifier], 訓練済みCatBoostモデルのリスト。\n",
    "    - X_test: pandas DataFrame, テストデータセット。\n",
    "    - columns_feature: list, 特徴量のカラム名のリスト。\n",
    "\n",
    "    戻り値:\n",
    "    - np.ndarray: テストデータセットの予測値の平均値。\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for model in models:\n",
    "        y_pred = model.predict_proba(X_test[columns_feature])[:, 1]\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "    predictions_mean = np.mean(predictions, axis=0)\n",
    "\n",
    "    return predictions_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_catboost(trial, X, y, columns_feature, n_splits=5):\n",
    "    \"\"\"\n",
    "    Optunaの試行に対する目的関数。CatBoostのハイパーパラメータを探索する。\n",
    "\n",
    "    引数:\n",
    "    - trial: optuna.trial.Trial オブジェクト\n",
    "    - X, y: 訓練データ\n",
    "    - columns_feature: 特徴量のカラム名のリスト\n",
    "    - n_splits: クロスバリデーションの分割数\n",
    "\n",
    "    戻り値:\n",
    "    - 試行の平均log lossスコア\n",
    "    \"\"\"\n",
    "    param = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"random_strength\": trial.suggest_int(\"random_strength\", 1, 20),\n",
    "        \"bagging_temperature\": trial.suggest_loguniform(\n",
    "            \"bagging_temperature\", 0.01, 1.0\n",
    "        ),\n",
    "        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 10),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 1, 255),\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"Logloss\",\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    _, scores = train_catboost_stratified_kfold(\n",
    "        X, y, columns_feature, [], param, n_splits\n",
    "    )\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def optimize_hyperparameters_catboost(\n",
    "    X, y, columns_feature, n_trials=100, n_splits=5, timeout=300\n",
    "):\n",
    "    \"\"\"\n",
    "    Optunaを使用してCatBoostモデルのハイパーパラメータを最適化する。\n",
    "\n",
    "    引数:\n",
    "    - X, y: 訓練データ\n",
    "    - columns_feature: 特徴量のカラム名のリスト\n",
    "    - n_trials: 試行回数の上限\n",
    "    - n_splits: クロスバリデーションの分割数\n",
    "    - timeout: 探索にかける時間の上限（秒）\n",
    "\n",
    "    戻り値:\n",
    "    - 最適なハイパーパラメータの辞書\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective_catboost(trial, X, y, columns_feature, n_splits),\n",
    "        n_trials=n_trials,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.value}\")\n",
    "    print(f\"Best params: {study.best_trial.params}\")\n",
    "\n",
    "    return study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "def train_random_forest_stratified_kfold(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    columns_feature: List[str],\n",
    "    target_binary: List[str],  # この実装では使用しないが、型情報を含める\n",
    "    param: Dict[str, any],\n",
    "    n_splits: int = 5,\n",
    ") -> Tuple[List[RandomForestClassifier], List[float]]:\n",
    "    \"\"\"\n",
    "    Stratified k-foldクロスバリデーションを使用してRandom Forestモデルを訓練する関数。\n",
    "    非均衡データを考慮して、各クラスの割合を保持する。\n",
    "\n",
    "    引数:\n",
    "    - X: pandas DataFrame, 特徴量データ。\n",
    "    - y: pandas Series, 目的変数データ。\n",
    "    - columns_feature: list, 特徴量のカラム名のリスト。\n",
    "    - target_binary: list, 目的変数のカラム名のリスト（この関数では使用しないが、一貫性のために残す）。\n",
    "    - param: dict, Random Forestモデルのパラメータ。\n",
    "    - n_splits: int, クロスバリデーションの分割数。\n",
    "\n",
    "    戻り値:\n",
    "    - Tuple[List[RandomForestClassifier], List[float]]: 訓練済みRandom Forestモデルのリストと各foldのlog lossスコアのリスト。\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    models: List[RandomForestClassifier] = []\n",
    "    scores: List[float] = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = (\n",
    "            X.iloc[train_index][columns_feature],\n",
    "            X.iloc[test_index][columns_feature],\n",
    "        )\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = RandomForestClassifier(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict_proba(X_test)[:, 1]\n",
    "        score = log_loss(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "        models.append(model)\n",
    "\n",
    "    return models, scores\n",
    "\n",
    "\n",
    "def predict_with_randomforest(\n",
    "    models: List[RandomForestClassifier],\n",
    "    X_test: pd.DataFrame,\n",
    "    columns_feature: List[str],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    訓練済みRandom Forestモデルのリストを使用してテストデータセットの予測を行う関数。\n",
    "\n",
    "    引数:\n",
    "    - models: List[RandomForestClassifier], 訓練済みRandom Forestモデルのリスト。\n",
    "    - X_test: pandas DataFrame, テストデータセット。\n",
    "    - columns_feature: list, 特徴量のカラム名のリスト。\n",
    "\n",
    "    戻り値:\n",
    "    - np.ndarray: テストデータセットの予測値の平均値。\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for model in models:\n",
    "        y_pred = model.predict_proba(X_test[columns_feature])[:, 1]\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "    predictions_mean = np.mean(predictions, axis=0)\n",
    "\n",
    "    return predictions_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_random_forest(trial, X, y, columns_feature, n_splits=5):\n",
    "    \"\"\"\n",
    "    Optunaの試行に対する目的関数。Random Forestのハイパーパラメータを探索する。\n",
    "\n",
    "    引数:\n",
    "    - trial: optuna.trial.Trial オブジェクト\n",
    "    - X, y: 訓練データ\n",
    "    - columns_feature: 特徴量のカラム名のリスト\n",
    "    - n_splits: クロスバリデーションの分割数\n",
    "\n",
    "    戻り値:\n",
    "    - 試行の平均log lossスコア\n",
    "    \"\"\"\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 150),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 60),\n",
    "        \"max_features\": trial.suggest_uniform(\"max_features\", 0.1, 1.0),\n",
    "    }\n",
    "\n",
    "    _, scores = train_random_forest_stratified_kfold(\n",
    "        X, y, columns_feature, [], param, n_splits\n",
    "    )\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def optimize_hyperparameters_random_forest(\n",
    "    X, y, columns_feature, n_trials=100, n_splits=5, timeout=300\n",
    "):\n",
    "    \"\"\"\n",
    "    Optunaを使用してRandom Forestモデルのハイパーパラメータを最適化する。\n",
    "\n",
    "    引数:\n",
    "    - X, y: 訓練データ\n",
    "    - columns_feature: 特徴量のカラム名のリスト\n",
    "    - n_trials: 試行回数の上限\n",
    "    - n_splits: クロスバリデーションの分割数\n",
    "    - timeout: 探索にかける時間の上限（秒）\n",
    "\n",
    "    戻り値:\n",
    "    - 最適なハイパーパラメータの辞書\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective_random_forest(trial, X, y, columns_feature, n_splits),\n",
    "        n_trials=n_trials,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.value}\")\n",
    "    print(f\"Best params: {study.best_trial.params}\")\n",
    "\n",
    "    return study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tabpfn_stratified_kfold(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    columns_feature: List[str],\n",
    "    target_binary: List[str],  # この実装では使用しないが、型情報を含める\n",
    "    n_splits: int = 5,\n",
    ") -> Tuple[List[TabPFNClassifier], List[float]]:\n",
    "    \"\"\"\n",
    "    Stratified k-foldクロスバリデーションを使用してTabPFNモデルを訓練する関数。\n",
    "\n",
    "    引数:\n",
    "    - X: pandas DataFrame, 特徴量データ。\n",
    "    - y: pandas Series, 目的変数データ。\n",
    "    - columns_feature: list, 特徴量のカラム名のリスト。\n",
    "    - target_binary: list, 目的変数のカラム名のリスト（この関数では使用しないが、一貫性のために残す）。\n",
    "    - n_splits: int, クロスバリデーションの分割数。\n",
    "\n",
    "    戻り値:\n",
    "    - Tuple[List[TabPFNClassifier], List[float]]: 訓練済みTabPFNモデルのリストと各foldのlog lossスコアのリスト。\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    models: List[TabPFNClassifier] = []\n",
    "    scores: List[float] = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index][columns_feature], X.iloc[test_index][columns_feature]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        classifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=8)\n",
    "        classifier.fit(X_train, y_train, overwrite_warning=True)\n",
    "        y_pred = classifier.predict_proba(X_test)[:, 1]  # 二値分類の場合\n",
    "\n",
    "        score: float = log_loss(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "        models.append(classifier)\n",
    "\n",
    "    return models, scores\n",
    "\n",
    "  \n",
    "def predict_with_tabpfn(\n",
    "    models: List[TabPFNClassifier],\n",
    "    X_test: pd.DataFrame,\n",
    "    columns_feature: List[str]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    訓練済みTabPFNClassifierモデルのリストを使用してテストデータセットの予測を行う関数。\n",
    "\n",
    "    引数:\n",
    "    - models: List[TabPFNClassifier], 訓練済みTabPFNClassifierモデルのリスト。\n",
    "    - X_test: pandas DataFrame, テストデータセット。\n",
    "    - columns_feature: list, 特徴量のカラム名のリスト。\n",
    "\n",
    "    戻り値:\n",
    "    - np.ndarray: テストデータセットの予測確率値の平均値。\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for model in models:\n",
    "        # TabPFNClassifierのpredict_probaメソッドを使用して確率を予測\n",
    "        # ここでは、すべてのクラスに対する確率を取得します\n",
    "        y_pred = model.predict_proba(X_test[columns_feature])\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "    # axis=0に沿って予測の平均を計算\n",
    "    predictions_mean = np.mean(predictions, axis=0)\n",
    "\n",
    "    return predictions_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aquamind-3o0A5rGC-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
